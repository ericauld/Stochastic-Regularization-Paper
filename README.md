# Stochastic-Regularization-Paper

This report summarizes work I did at UCLA while I was pursuing my Ph.D. It is about stochastic regularizers on neural networks, in particular the "batch normalization" and "dropout" methods, and investigating their compatibility and attempting to clarify the means by which they regularize (and perhaps speed up optimization as well).

The accompanying code badly needs refactoring, so it has been taken down temporarily. It is available upon request.
