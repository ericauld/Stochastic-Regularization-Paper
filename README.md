# Stochastic-Regularization-Paper

This report summarizes work I did at UCLA while I was pursuing my Ph.D. It is about stochastic regularizers on neural networks, in particular the "batch normalization" and "dropout" methods. It investigates the compatibility of the two methods and attempts to clarify how they regularize (and how they perhaps improve optimization speed as well).

The accompanying code badly needs refactoring, so it has been taken down temporarily. It is available upon request.
